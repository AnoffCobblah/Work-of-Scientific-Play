---
title: "04-RR-RoyInst"
author: "Anoff Nicholas Cobblah"
date: "July 31, 2018"
output: html_document
  html_document:
    number_sections: yes
    toc: true
    toc_depth: 6
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# July 2020: Play and Work Terms in 19th-Century Royal Institution

This script combines my Word Flagging and KWIC (tokenizer script) methods in order to create an interactive illustration of the frequency with which the lemma "amus", "plai", "play", "player", "recreat","labor","labori", "labour", "toil", "work", and "worker" were referenced in 19th-century volumes of the *Notices of the Proceedings at the Meetings of the Members of the Royal Institution.* The goal is to determine whether references to work and play make up a larger proportion of the corpus at the end of the century than at the beginning, and to visualize this in such a way that scrolling over a point automatically produces a key words in context (randomly).

First we load our libraries and set the parameters.

**IMPORTANT NOTE: Since creating a Word Flag matrix can take a nontrivial amount of time for larger corpuses, this script is designed only to run the program to create a new RoyInstWordFlagdf if there is a change to the dataset in folder "Documents" or if the previous RoyInstWordFlagdf has been deleted.**

```{r,  eval=FALSE}
    library(SnowballC)
    library(ggplot2)
    library(tokenizers)
    library(readr)
    library(plotly)
    RoyInstlocation <- paste0(getwd())
    RoyInstdoclocation <- paste0(RoyInstlocation,"/Corpora/SciOrgs/RoyInst/Proceedings")
    RoyInstlongconlength <- 250
    RoyInstshortconlength <- 3
    RoyInstPOSconlength <- 10
    AmusementTerms <- c("amus")
    PlayTerms <- c("plai", "play", "player")
    RecreationTerms <- c("recreat")
    LaborTerms <- c("labor","labori", "labour")
    ToilTerms <- c("toil")
    WorkTerms <- c("work", "worker")
    Categories <- c("Amusement","Play","Recreation","Labor","Toil","Work")
    RoyInststemsearchedtermlist <- c(AmusementTerms,PlayTerms,RecreationTerms,LaborTerms,ToilTerms,WorkTerms)
    RoyInstoutputlocation <- paste0(getwd(),"/WordFlagDataFrames")
    RoyInstWordFlagdfPath <- paste0(RoyInstoutputlocation,"/","RoyInstWordFlagdf.txt")
    RoyInstDocumentSize <- 31964986
```

To create the data frame compiling every reference to a term, or load in the previous data frame if nothing has changed, we run the following script.

```{r DecRoyInstApp Word Flag,  eval=FALSE}
      if(sum(file.info(list.files(RoyInstdoclocation, all.files = TRUE, recursive = TRUE, full.names=TRUE))$size) == RoyInstDocumentSize) {
        RoyInstDataChange1 <- FALSE
        print("The data in the 'Documents' folder appears not to have changed.")
      }else{
        RoyInstDataChange1 <- TRUE
        print("The data in the 'Documents' folder appears to have been changed. A new RoyInstWordFlagdf will therefore be created. TO UPDATE THIS SCRIPT, PLEASE CHANGE THE RoyInstDocumentSize TO REFLECT THE NEW SIZE OF THE DOCUMENTS.")
        }
      
      if(file.exists(RoyInstWordFlagdfPath) == TRUE) {
        RoyInstDataChange2 <- FALSE
        print("The previous RoyInstWordFlagdf still exists.")
      }else{
        RoyInstDataChange2 <- TRUE
        print("The previous RoyInstwordFlagdf seems to have been moved or deleted.  A new RoyInstWordFlag will therefore be created.")
        }

  if(RoyInstDataChange1|RoyInstDataChange2 == TRUE) {
  
      files <- list.files(path = RoyInstdoclocation, pattern = "txt", full.names = TRUE) #creates vector of txt file names.
      if(file.exists(RoyInstoutputlocation) == FALSE){dir.create(RoyInstoutputlocation)}
      RoyInstWordFlagmat <- matrix(,ncol=12,nrow=1)
      for (i in 1:length(files)) {
        fileName <- read_file(files[i])
        Encoding(fileName) <- "UTF-8"  #since tokenize_sentences function requires things to be encoded in UTF-8, need to remove some data.
        fileName <- iconv(fileName, "UTF-8", "UTF-8",sub='')
        ltoken <- tokenize_words(fileName, lowercase = TRUE, stopwords = NULL, simplify = FALSE)
        ltoken <- unlist(ltoken)
        stemltoken <- wordStem(ltoken) #this uses the Snowball library to lemmatize the entire text.
        textID <- i
        for (p in 1:length(RoyInststemsearchedtermlist)) {
          RoyInststemsearchedterm <- RoyInststemsearchedtermlist[p]
          for (j in 1:length(stemltoken)) {
              if (RoyInststemsearchedterm == stemltoken[j]) {
                if (j <= RoyInstlongconlength) {longtempvec <- ltoken[(1:(j+RoyInstlongconlength))]}
                if (j > RoyInstlongconlength) {longtempvec <- ltoken[(j-RoyInstlongconlength):(j+RoyInstlongconlength)]}
                if (j <= RoyInstshortconlength) {shorttempvec <- ltoken[(1:(j+RoyInstshortconlength))]}
                if (j > RoyInstshortconlength) {shorttempvec <- ltoken[(j-RoyInstshortconlength):(j+RoyInstshortconlength)]}
                if (j <= RoyInstPOSconlength) {POStempvec <- ltoken[(1:(j+RoyInstPOSconlength))]}
                if (j > RoyInstPOSconlength) {POStempvec <- ltoken[(j-RoyInstPOSconlength):(j+RoyInstPOSconlength)]}
                TempTextName <- gsub(paste0(RoyInstdoclocation,"/"),"",files[i]) #This grabs just the end of the file path.
                TempTextName <- gsub(".txt","",TempTextName) #This removes the .txt from the end of the name.
                temprow <- matrix(,ncol=12,nrow=1)
                colnames(temprow) <- c("Text", "Text_ID", "RoyInststemsearchedterm","Lemma","Lemma_Perc","KWIC","Total_Lemma","Date","Category","Short_KWIC","POS_KWIC","Current_Date")
                temprow[1,1] <- TempTextName
                temprow[1,2] <- textID
                temprow[1,3] <- RoyInststemsearchedterm
                temprow[1,4] <- j
                temprow[1,5] <- (j/length(stemltoken))*100
                temprow[1,6] <- as.character(paste(longtempvec,sep= " ",collapse=" "))
                temprow[1,7] <- length(stemltoken)
                temprow[1,8] <- strsplit(TempTextName,"_")[[1]][1]
                #Determining Category
                  if(RoyInststemsearchedterm %in% AmusementTerms) {temprow[1,9] <- "Amusement"}
                  if(RoyInststemsearchedterm %in% PlayTerms) {temprow[1,9] <- "Play"}
                  if(RoyInststemsearchedterm %in% RecreationTerms) {temprow[1,9] <- "Recreation"}
                  if(RoyInststemsearchedterm %in% LaborTerms) {temprow[1,9] <- "Labor"}
                  if(RoyInststemsearchedterm %in% ToilTerms) {temprow[1,9] <- "Toil"}
                  if(RoyInststemsearchedterm %in% WorkTerms) {temprow[1,9] <- "Work"}
                temprow[1,10] <- as.character(paste(shorttempvec,sep= " ",collapse=" "))
                temprow[1,11] <- as.character(paste(POStempvec,sep= " ",collapse=" "))
                temprow[1,12] <- format(Sys.time(), "%Y-%m-%d")
                RoyInstWordFlagmat <- rbind(RoyInstWordFlagmat,temprow)
              }
          }
        }
        print(files[i]) #let's user watch as code runs for long searches
      }
      RoyInstWordFlagmat <- RoyInstWordFlagmat[-1,]
      RoyInstWordFlagdf <- as.data.frame(RoyInstWordFlagmat)
      write.table(RoyInstWordFlagdf, RoyInstWordFlagdfPath)
      RoyInstWordFlagdf[1:5,]
  }else{
    print("Loading the previous dataset as RoyInstWordFlagdf")
    RoyInstWordFlagdf <- read.table(RoyInstWordFlagdfPath)
  }
RoyInstWordFlagdf
```

We can then add up the values in SciLifeWordFlagdf to make a table of the frequency of play and work terms for each text: RoyInstFreqmat.Again, it's important to do it this way because it lets us assign a random KWIC for later.

```{r,  eval=FALSE}
  # Adding values from RoyInstWordFlagdf together to get a matrix of normalized frequencies for each category, as RoyInstFreqmat

      RoyInstFreqmat <- matrix(,ncol=9,nrow=1)
      files <- list.files(path = RoyInstdoclocation, pattern = "txt", full.names = TRUE) #creates vector of txt file names.
    for (i in 1:length(files)) {
      TempTextName <- gsub(paste0(RoyInstdoclocation,"/"),"",files[i]) #This grabs just the end of the file path.
      TempTextName <- gsub(".txt","",TempTextName) #This removes the .txt from the end of the name.
      TempDate <- strsplit(TempTextName,"_")[[1]][1]
      
      tempdf <- RoyInstWordFlagdf[grep(TempTextName,RoyInstWordFlagdf$Text),]
      TempLength <- unique(tempdf$Total_Lemma) #note that this unique is here to make SURE we only have one text
      
      for (z in 1:length(Categories)) {   
        tempdf2 <- tempdf[grep(Categories[z],tempdf$Category),]
        
        temprows <- matrix(,ncol=9,nrow=1)
        colnames(temprows) <- c("Text", "Text_ID","Date","Category","Frequency","Total_Lemma","Normalized_Freq","Sample_KWIC","Avg_Lemma_Perc")
        temprows[1,1] <- as.character(TempTextName)
        temprows[1,2] <- i
        temprows[1,3] <- as.character(TempDate)
        temprows[1,4] <- Categories[z]
        temprows[1,5] <- nrow(tempdf2)
        temprows[1,6]<- as.character(TempLength)
        temprows[1,7] <- (as.numeric(temprows[1,5])/as.numeric(temprows[1,6]))*100
        #temprows[1,8]
          if(nrow(tempdf2) > 0){temprows[1,8] <- as.character(sample(tempdf2$Short_KWIC,1))}else{temprows[1,8] <- NA}
        temprows[1,9] <- mean(as.numeric(as.character(tempdf2$Lemma_Perc)))
        RoyInstFreqmat <- rbind(RoyInstFreqmat,temprows)
      }
    }
    RoyInstFreqmat <- RoyInstFreqmat[-1,]
    RoyInstFreqdf <- as.data.frame(RoyInstFreqmat)
    RoyInstFreqdf
      
```

With the data in hand, we can make an interactive plot to see the normalized frequencies for each text, and a random KWIC. This helps us see whether our data seems like it makes sense.

```{r,  eval=FALSE}
# Visualizing RoyInstFreqdf BY DATE
      p <- ggplot(RoyInstFreqdf, aes(y = as.numeric(as.character(Normalized_Freq)), x = as.numeric(substr(Date,1,4)), color = Category, label = Sample_KWIC))
      pg <- geom_point(size=1,pch = 16)
      pl <- p + pg + labs(x = "Date", y = "Normalized Frequency (% of Words in Text)", title = "Appearances of Play and Work Rhetoric within 19th-Century Popular Science Corpus")
      ggplotly(pl)
```

Once we've verified that the data within texts looks right, we can do the same, but per decade, by creating a new dataframe: RoyInstFreqDecadedf.

```{r,  eval=FALSE}
  # Adding values from RoyInstFreqdf together to get a matrix of normalized frequencies for each decade, as RoyInstFreqDecadedf

      RoyInstFreqDecadeMat <- matrix(,ncol=6,nrow=1)
      
    Decades <- c(180, 181, 182, 183, 184, 185, 186, 187, 188, 189)
    for (z in 1:length(Categories)) { 
      tempdf <- RoyInstFreqdf[grep(Categories[z],RoyInstFreqdf$Category),]
      
      for (i in 1:length(Decades)) {
        TempDecade <- paste0(Decades[i],"0s")
        tempdf2 <- tempdf[grep(Decades[i],tempdf$Date),]
        temprows <- matrix(,ncol=6,nrow=1)
        colnames(temprows) <- c("Decade","Category","Frequency","Total_Lemma","Normalized_Freq","Sample_KWIC")
        temprows[1,1] <- as.character(TempDecade)
        temprows[1,2] <- Categories[z]
        temprows[1,3] <- sum(as.numeric(as.character(tempdf2$Frequency)), na.rm=TRUE)
        temprows[1,4] <- sum(as.numeric(as.character(tempdf2$Total_Lemma)), na.rm=TRUE)
        temprows[1,5] <- (as.numeric(temprows[1,3])/as.numeric(temprows[1,4]))*100
        #temprows[1,6]
          if(nrow(tempdf2) > 0){temprows[1,6] <- as.character(sample(tempdf2$Sample_KWIC,1))}else{temprows[1,6] <- NA}
        RoyInstFreqDecadeMat <- rbind(RoyInstFreqDecadeMat,temprows)
      }
    }
    RoyInstFreqDecadeMat <- RoyInstFreqDecadeMat[-1,]
    RoyInstFreqDecadedf <- as.data.frame(RoyInstFreqDecadeMat)
    RoyInstFreqDecadedf
      
```

Now we can visualize this for each category by decade, narrowing our decades to just the nineteenth-century.

```{r,  eval=FALSE}
#set a category.
z=6
tempdf <- RoyInstFreqDecadedf[grep(Categories[z],RoyInstFreqDecadedf$Category),]

# Visualizing the category BY decade. Note that there is no y axis because it kept running into the numbers, and trying to fix it was becoming a pain.
      p <- ggplot(tempdf, aes(y = as.numeric(as.character(Normalized_Freq)), x = Decade, label = Sample_KWIC))
      pg <- geom_bar(stat="identity",position="dodge")
      pl <- p + pg + xlab("Decade") +ylab("Normalized Frequency (%)") + ggtitle(paste0("Normalized Frequency of '",Categories[z],"' by Decade\n in the Notices of the Royal Institution"))+theme( axis.text.y=element_text(angle=90))
      ggplotly(pl)
```