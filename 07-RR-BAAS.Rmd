---
title: "07-RR-BAAS"
author: "Anoff Nicholas Cobblah"
date: "July 31, 2018"
output: html_document
  html_document:
    number_sections: yes
    toc: true
    toc_depth: 6
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# July 2020: Play and Work Terms in 19th-Century *Reports of the BAAS*

This script combines my Word Flagging and KWIC (tokenizer script) methods in order to create an interactive illustration of the frequency with which the lemma "amus", "plai", "play", "player", "recreat","labor","labori", "labour", "toil", "work", and "worker" were referenced in 19th-century *Reports of the BAAS*. The goal is to determine whether references to work and play make up a larger proportion of the corpus at the end of the century than at the beginning, and to visualize this in such a way that scrolling over a point automatically produces a key words in context (randomly).

First we load our libraries and set the parameters.

**IMPORTANT NOTE: Since creating a Word Flag matrix can take a nontrivial amount of time for larger corpuses, this script is designed only to run the program to create a new BAASWordFlagdf if there is a change to the dataset in folder "Documents" or if the previous BAASWordFlagdf has been deleted.**

```{r,  eval=FALSE}
    library(SnowballC)
    library(ggplot2)
    library(tokenizers)
    library(readr)
    library(plotly)
    BAASlocation <- paste0(getwd())
    BAASdoclocation <- paste0(BAASlocation,"/Corpora/SciOrgs/BAAS/Reports")
    BAASlongconlength <- 250
    BAASshortconlength <- 3
    BAASPOSconlength <- 10
    AmusementTerms <- c("amus")
    PlayTerms <- c("plai", "play", "player")
    RecreationTerms <- c("recreat")
    LaborTerms <- c("labor","labori", "labour")
    ToilTerms <- c("toil")
    WorkTerms <- c("work", "worker")
    Categories <- c("Amusement","Play","Recreation","Labor","Toil","Work")
    BAASstemsearchedtermlist <- c(AmusementTerms,PlayTerms,RecreationTerms,LaborTerms,ToilTerms,WorkTerms)
    BAASoutputlocation <- paste0(getwd(),"/WordFlagDataFrames")
    BAASWordFlagdfPath <- paste0(BAASoutputlocation,"/","BAASWordFlagdf.txt")
    BAASDocumentSize <- 210370764
```

To create the data frame compiling every reference to a term, or load in the previous data frame if nothing has changed, we run the following script.

```{r DecBAASApp Word Flag,  eval=FALSE}
      if(sum(file.info(list.files(BAASdoclocation, all.files = TRUE, recursive = TRUE, full.names=TRUE))$size) == BAASDocumentSize) {
        BAASDataChange1 <- FALSE
        print("The data in the 'Documents' folder appears not to have changed.")
      }else{
        BAASDataChange1 <- TRUE
        print("The data in the 'Documents' folder appears to have been changed. A new BAASWordFlagdf will therefore be created. TO UPDATE THIS SCRIPT, PLEASE CHANGE THE BAASDocumentSize TO REFLECT THE NEW SIZE OF THE DOCUMENTS.")
        }
      
      if(file.exists(BAASWordFlagdfPath) == TRUE) {
        BAASDataChange2 <- FALSE
        print("The previous BAASWordFlagdf still exists.")
      }else{
        BAASDataChange2 <- TRUE
        print("The previous BAASwordFlagdf seems to have been moved or deleted.  A new BAASWordFlag will therefore be created.")
        }

  if(BAASDataChange1|BAASDataChange2 == TRUE) {
  
      files <- list.files(path = BAASdoclocation, pattern = "txt", full.names = TRUE) #creates vector of txt file names.
      if(file.exists(BAASoutputlocation) == FALSE){dir.create(BAASoutputlocation)}
      BAASWordFlagmat <- matrix(,ncol=12,nrow=1)
      for (i in 1:length(files)) {
        fileName <- read_file(files[i])
        Encoding(fileName) <- "UTF-8"  #since tokenize_sentences function requires things to be encoded in UTF-8, need to remove some data.
        fileName <- iconv(fileName, "UTF-8", "UTF-8",sub='')
        ltoken <- tokenize_words(fileName, lowercase = TRUE, stopwords = NULL, simplify = FALSE)
        ltoken <- unlist(ltoken)
        stemltoken <- wordStem(ltoken) #this uses the Snowball library to lemmatize the entire text.
        textID <- i
        for (p in 1:length(BAASstemsearchedtermlist)) {
          BAASstemsearchedterm <- BAASstemsearchedtermlist[p]
          for (j in 1:length(stemltoken)) {
              if (BAASstemsearchedterm == stemltoken[j]) {
                if (j <= BAASlongconlength) {longtempvec <- ltoken[(1:(j+BAASlongconlength))]}
                if (j > BAASlongconlength) {longtempvec <- ltoken[(j-BAASlongconlength):(j+BAASlongconlength)]}
                if (j <= BAASshortconlength) {shorttempvec <- ltoken[(1:(j+BAASshortconlength))]}
                if (j > BAASshortconlength) {shorttempvec <- ltoken[(j-BAASshortconlength):(j+BAASshortconlength)]}
                if (j <= BAASPOSconlength) {POStempvec <- ltoken[(1:(j+BAASPOSconlength))]}
                if (j > BAASPOSconlength) {POStempvec <- ltoken[(j-BAASPOSconlength):(j+BAASPOSconlength)]}
                TempTextName <- gsub(paste0(BAASdoclocation,"/"),"",files[i]) #This grabs just the end of the file path.
                TempTextName <- gsub(".txt","",TempTextName) #This removes the .txt from the end of the name.
                temprow <- matrix(,ncol=12,nrow=1)
                colnames(temprow) <- c("Text", "Text_ID", "BAASstemsearchedterm","Lemma","Lemma_Perc","KWIC","Total_Lemma","Date","Category","Short_KWIC","POS_KWIC","Current_Date")
                temprow[1,1] <- TempTextName
                temprow[1,2] <- textID
                temprow[1,3] <- BAASstemsearchedterm
                temprow[1,4] <- j
                temprow[1,5] <- (j/length(stemltoken))*100
                temprow[1,6] <- as.character(paste(longtempvec,sep= " ",collapse=" "))
                temprow[1,7] <- length(stemltoken)
                temprow[1,8] <- strsplit(TempTextName,"_")[[1]][1]
                #Determining Category
                  if(BAASstemsearchedterm %in% AmusementTerms) {temprow[1,9] <- "Amusement"}
                  if(BAASstemsearchedterm %in% PlayTerms) {temprow[1,9] <- "Play"}
                  if(BAASstemsearchedterm %in% RecreationTerms) {temprow[1,9] <- "Recreation"}
                  if(BAASstemsearchedterm %in% LaborTerms) {temprow[1,9] <- "Labor"}
                  if(BAASstemsearchedterm %in% ToilTerms) {temprow[1,9] <- "Toil"}
                  if(BAASstemsearchedterm %in% WorkTerms) {temprow[1,9] <- "Work"}
                temprow[1,10] <- as.character(paste(shorttempvec,sep= " ",collapse=" "))
                temprow[1,11] <- as.character(paste(POStempvec,sep= " ",collapse=" "))
                temprow[1,12] <- format(Sys.time(), "%Y-%m-%d")
                BAASWordFlagmat <- rbind(BAASWordFlagmat,temprow)
              }
          }
        }
        print(files[i]) #let's user watch as code runs for long searches
      }
      BAASWordFlagmat <- BAASWordFlagmat[-1,]
      BAASWordFlagdf <- as.data.frame(BAASWordFlagmat)
      write.table(BAASWordFlagdf, BAASWordFlagdfPath)
      BAASWordFlagdf[1:5,]
  }else{
    print("Loading the previous dataset as BAASWordFlagdf")
    BAASWordFlagdf <- read.table(BAASWordFlagdfPath)
  }
BAASWordFlagdf
```

We can then add up the values in SciLifeWordFlagdf to make a table of the frequency of play and work terms for each text: BAASFreqmat.Again, it's important to do it this way because it lets us assign a random KWIC for later.

```{r,  eval=FALSE}
  # Adding values from BAASWordFlagdf together to get a matrix of normalized frequencies for each category, as BAASFreqmat

      BAASFreqmat <- matrix(,ncol=9,nrow=1)
      files <- list.files(path = BAASdoclocation, pattern = "txt", full.names = TRUE) #creates vector of txt file names.
    for (i in 1:length(files)) {
      TempTextName <- gsub(paste0(BAASdoclocation,"/"),"",files[i]) #This grabs just the end of the file path.
      TempTextName <- gsub(".txt","",TempTextName) #This removes the .txt from the end of the name.
      TempDate <- strsplit(TempTextName,"_")[[1]][1]
      
      tempdf <- BAASWordFlagdf[grep(TempTextName,BAASWordFlagdf$Text),]
      TempLength <- unique(tempdf$Total_Lemma) #note that this unique is here to make SURE we only have one text
      
      for (z in 1:length(Categories)) {   
        tempdf2 <- tempdf[grep(Categories[z],tempdf$Category),]
        
        temprows <- matrix(,ncol=9,nrow=1)
        colnames(temprows) <- c("Text", "Text_ID","Date","Category","Frequency","Total_Lemma","Normalized_Freq","Sample_KWIC","Avg_Lemma_Perc")
        temprows[1,1] <- as.character(TempTextName)
        temprows[1,2] <- i
        temprows[1,3] <- as.character(TempDate)
        temprows[1,4] <- Categories[z]
        temprows[1,5] <- nrow(tempdf2)
        temprows[1,6]<- as.character(TempLength)
        temprows[1,7] <- (as.numeric(temprows[1,5])/as.numeric(temprows[1,6]))*100
        #temprows[1,8]
          if(nrow(tempdf2) > 0){temprows[1,8] <- as.character(sample(tempdf2$Short_KWIC,1))}else{temprows[1,8] <- NA}
        temprows[1,9] <- mean(as.numeric(as.character(tempdf2$Lemma_Perc)))
        BAASFreqmat <- rbind(BAASFreqmat,temprows)
      }
    }
    BAASFreqmat <- BAASFreqmat[-1,]
    BAASFreqdf <- as.data.frame(BAASFreqmat)
    BAASFreqdf
      
```

With the data in hand, we can make an interactive plot to see the normalized frequencies for each text, and a random KWIC. This helps us see whether our data seems like it makes sense.

```{r,  eval=FALSE}
# Visualizing BAASFreqdf BY DATE
      p <- ggplot(BAASFreqdf, aes(y = as.numeric(as.character(Normalized_Freq)), x = as.numeric(substr(Date,1,4)), color = Category, label = Sample_KWIC))
      pg <- geom_point(size=1,pch = 16)
      pl <- p + pg + labs(x = "Date", y = "Normalized Frequency (% of Words in Text)", title = "Appearances of Play and Work Rhetoric within 19th-Century Popular Science Corpus")
      ggplotly(pl)
```

Once we've verified that the data within texts looks right, we can do the same, but per decade, by creating a new dataframe: BAASFreqDecadedf.

```{r,  eval=FALSE}
  # Adding values from BAASFreqdf together to get a matrix of normalized frequencies for each decade, as BAASFreqDecadedf

      BAASFreqDecadeMat <- matrix(,ncol=6,nrow=1)
      
    Decades <- c(180, 181, 182, 183, 184, 185, 186, 187, 188, 189)
    for (z in 1:length(Categories)) { 
      tempdf <- BAASFreqdf[grep(Categories[z],BAASFreqdf$Category),]
      
      for (i in 1:length(Decades)) {
        TempDecade <- paste0(Decades[i],"0s")
        tempdf2 <- tempdf[grep(Decades[i],tempdf$Date),]
        temprows <- matrix(,ncol=6,nrow=1)
        colnames(temprows) <- c("Decade","Category","Frequency","Total_Lemma","Normalized_Freq","Sample_KWIC")
        temprows[1,1] <- as.character(TempDecade)
        temprows[1,2] <- Categories[z]
        temprows[1,3] <- sum(as.numeric(as.character(tempdf2$Frequency)), na.rm=TRUE)
        temprows[1,4] <- sum(as.numeric(as.character(tempdf2$Total_Lemma)), na.rm=TRUE)
        temprows[1,5] <- (as.numeric(temprows[1,3])/as.numeric(temprows[1,4]))*100
        #temprows[1,6]
          if(nrow(tempdf2) > 0){temprows[1,6] <- as.character(sample(tempdf2$Sample_KWIC,1))}else{temprows[1,6] <- NA}
        BAASFreqDecadeMat <- rbind(BAASFreqDecadeMat,temprows)
      }
    }
    BAASFreqDecadeMat <- BAASFreqDecadeMat[-1,]
    BAASFreqDecadedf <- as.data.frame(BAASFreqDecadeMat)
    BAASFreqDecadedf
      
```

Now we can visualize this for each category by decade, narrowing our decades to just the nineteenth-century.

```{r,  eval=FALSE}
#set a category.
z=6
tempdf <- BAASFreqDecadedf[grep(Categories[z],BAASFreqDecadedf$Category),]

# Visualizing the category BY decade. Note that there is no y axis because it kept running into the numbers, and trying to fix it was becoming a pain.
      p <- ggplot(tempdf, aes(y = as.numeric(as.character(Normalized_Freq)), x = Decade, label = Sample_KWIC))
      pg <- geom_bar(stat="identity",position="dodge")
      pl <- p + pg + xlab("Decade") +ylab("Normalized Frequency (%)") + ggtitle(paste0("Normalized Frequency of '",Categories[z],"' by Decade\n in the Reports of the BAAS (1834-1900)"))+theme( axis.text.y=element_text(angle=90))
      ggplotly(pl)
```